Title: Perhaps a related question is how easy is to take ...
Date: 2010-11-24 22:14
Author: spiv (noreply@blogger.com)
Slug: big-or-small/8607739841621494036

Perhaps a related question is how easy is to take a many small code
bases and bundle them as one later, versus separating one big code base
into many small ones? I think small has a slight advantage over big
here: in general the pain I've observed in e.g. building bzr's windows
installer or various core+plugins PPAs has probably been a little less
than Twisted's pain in producing releases of individual subpackages. But
I'm not sure, and perhaps the tradeoffs vary quite a lot between
projects.  
  
I'm frankly deeply frightened by “scaling problems with big projects”
though. To focus on just one aspect of that, I'm increasingly feeling
that increased complexity brings massive, perhaps exponential, increase
in cost. Just compare how easy it is to do a quick hack *and feel that
it is good enough* in a small project versus a large one. For a mini
case study, look at ControlDir.sprout in current bzrlib: over time we've
added features like separate metadirs, shared repositories, stacking
policies, hardlinking, reusing transport objects, subtrees,
optimisations to the way we do a fetch into a newly sprouted
repository... and each of those things has taken a toll on this one
function. All nice things to have, but now it alarmingly difficult to
make further improvements to this function because complexity is fragile
— happily I mostly trust the test suite here, which helps, but only the
full test suite. And of course I \*do\* want to improve that
function...  
  
In the case of ControlDir.sprout I'm hopeful that some refactorings may
ease the pain a little, but I think truly radical surgery is required if
it is going to be anything other than hideous.  
  
So I guess I'd lean towards many small over one big, but try to find
ways to reduce the duplication release effort and project
infrastructure.

